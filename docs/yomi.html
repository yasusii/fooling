<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=euc-jp">
<title>
視覚障害者のための読みがなによる全文検索システム、その提案と実装
</title>
<style type="text/css">
BODY { line-height: 130%; }
BLOCKQUOTE { background: #eeeeee; }
CODE { background: #eeeeee; }
KBD { background: #eeeeee; font-weight: bold; }
</style>
</head>
<body>
<h2>
視覚障害者のための読みがなによる全文検索システム、その提案と実装
</h2>

<h3>概要</h3>
<p>
本記事では視覚障害者の利用を想定した、
漢字をふくむ日本語文字列を読みがなだけで検索するシステムを提案する。
本手法は bi-gram と動的計画法をもちいて、漢字を含む文字列を
すべての可能な読み方で効率的にインデックス、および検索することを可能にする。
試験的な実装により、本手法が大規模な文書検索に応用可能であること、
また純粋な読みがなだけの検索では不十分な場合があることを示す。

<ul>
<li> 1. <a href="#background">背景</a>
<li> 2. <a href="#basic">基本的な機構</a>
<li> 2.1. <a href="#overall">検索の流れ</a>
<li> 3. <a href="#indexing">インデクシングのアルゴリズム</a>
<li> 4. <a href="#matching">マッチングのアルゴリズム</a>
<li> 5. <a href="#optimization">辞書ファイルの最適化</a>
<li> 6. <a href="#braille">点字規則の追加</a>
<li> 7. <a href="#exp">評価実験</a>
<li> 7.1. <a href="#impl">実装</a>
<li> 7.2. <a href="#docset">実験に用いた文書集合</a>
<li> 7.3. <a href="#indexingtime">インデックシング</a>
<li> 7.4. <a href="#searchingtime">検索</a>
<li> 8. <a href="#future">残された課題</a>
<li> 8.1. <a href="#filtering">検索結果のランキングおよびフィルタリング</a>
<li> 8.2. <a href="#varied">読みがな以外による表記のゆれ</a>
<li> 8.3. <a href="#efficiency">検索効率の改善</a>
</ul>

<hr noshade>
<a name="background"><h3>1. 背景</h3></a>
<p>
現在の視覚障害者は、おもに点字ディスプレイまたは
音声合成をもちいた画面読み上げソフトウェア (スクリーンリーダ) を用いて
コンピュータにアクセスしている。しかし現在普及している点字は
ほとんどが表音文字であり (注: 点字をもちいて漢字を表現する 
6点漢字あるいは 8点漢字という手法も提案されているが、
広く普及するには至っていない)、
画面読み上げソフトウェアもまた音声に頼っていることから、
視覚障害者が日本語の漢字表記に触れる機会は晴眼者に比べて少ない。
このことは、コンピュータを使って大規模な文書検索をおこなうさいに
問題となる。現在普及している、文字列一致による検索システムでは
ユーザは検索したい単語がどのような綴りなのか、あるいはそもそもそれが漢字
(あるいはカタカナ) によって表現されているのかどうかを
あらかじめ知っている必要がある。
さらに複数の単語からなる文や句全体を検索する際には、
ユーザはそれが (一般的な かな漢字変換ソフトウェアを用いて) どのように
漢字に変換されるかもある程度予測できていなければならない。
このようなことは、漢字表記を普段からあまり意識していない
視覚障害のユーザーにとって困難である。高齢の視覚障害者の場合、
ユーザは漢字をほとんど知らないこともある。そこで、
本記事では漢字 (あるいはカタカナ) を含む文字列の「読み」だけで
大規模な文書集合を効率よく検索できるような機構を提案する。
<p>
検索システムを設計する立場から考えると、この問題は検索対象となる
文字列に含まれる漢字すべてに正しい読み方を付与すれば解決できるように思われる。
しかし、すべての漢字、とりわけ日本語の固有名詞に対して
一意の読み方を付与することは難しい。固有名詞は同じ漢字表記であっても
さまざまな読み方が存在し、適切な文脈情報なしには読みを決定することは
不可能である。たとえば「<code>新山</code>」という文字列には、
「<code>にいやま</code>」「<code>しんやま</code>」「<code>しんざん</code>」などの可能な読み方が存在する。
また、たとえ正しい読みが付与できたとしても、ユーザがその読み方を
知らないこともありうる。なぜなら一般的な音声合成技術を使っている
現在のスクリーンリーダは、まさに上で示した理由により、固有名詞を正しく
発音しない場合が多いからである。
そこで本記事で提案する検索システムでは、ある漢字かな混じり文字列を
可能なすべての読みで検索できるようにすることを目標とする。
しかし、伝統的な手法を用いてこのような検索を実現しようとすると
計算量およびデータ量の増大という問題が発生する。
たとえば「<code>日本の祭事</code>」という文字列を考えてみよう。
「日本」という文字列は「にほん」とも「にっぽん」とも読めるし、
また「祭事」は、「さいじ」あるいは「まつりごと」とも読めるので、
少なくとも「日本の祭事」に対しては「にほんのさいじ」「にっぽんのさいじ」
「にほんのまつりごと」「にっぽんのまつりごと」の 4種類の読み方が存在する。
このような読み方をすべて列挙すると、一般的に可能性のある読み方は
連続する漢字の数に応じて指数的に増えてしまう。
<P>
本記事では、2-グラム (bi-gram) と動的計画法 (dynamic programming) を
もちいて、これらの可能な読みをすべて現実的な計算時間で処理する
手法を提案する。また、この機構を既存のオープンソース
検索システムである 
<a href="http://www.unixuser.org/~euske/python/fooling/index.html">Fooling</a>
に実装し、簡単な性能評価をおこなった。
本手法は、文書をすべてひらがな (あるいはカタカナ) による
読みで検索するが、オプションとしてローマ字入力による検索も可能である。
したがって、この手法は将来的に文字入力のためのキーが限られている
携帯デバイスなどからの文書検索システムにも応用できるかもしれない。

<hr noshade>
<a name="basic"><h3>2. 基本的な機構</h3></a>
<p>
本システムでは、文字列の検索は 2つの段階に分けられる。
ひとつは、検索の絞りこみのためのデータ構造 (インデックス、index) を
検索対象となる文書集合からあらかじめ作成しておく「インデクシング」であり、
もうひとつは実際に与えられた検索要求 (読みがな) に
適合する文書を抽出する「検索処理」である。後述するように、
本システムでは検索処理はさらに 2つの段階
(「絞りこみ」と「マッチング」) に分けられる。
<p>
一般的な文書検索システムにおいては、インデックスは
ユーザが入力する各検索語をふくんだ文書集合を対応づける写像に
なっている。インデックスは、すべての可能な検索キーワードに対して、
それらが含まれる文書の集合を対応づけるような写像であるが、
返される文書が必ずしもすべて検索結果に完全に一致する必要はない。
インデックスは検索要求に一致する可能性が (他の文書に比べて) 
十分に高いようなものを、適当な数にまで絞りこむために使われる。
本システムはこのアイデアにもとづき、インデックスのキーとして
読みがなの bi-gram を使う。読みがなの bi-gram とは、日本語の
読みがなのうち連続する 2文字をそれぞれ取り出したものである。
たとえば「<code>明後日</code>」という文字列に対応する読みがなは、
「あさって」であるが、これは
「<code>あさ</code>」「<code>さっ</code>」「<code>って</code>」という 3つの bi-gram に分解できる。
本システムでは、このような読みがなの対応は辞書ファイルによって与えられる。
なお、実際には本システムではインデックスする単位は文書ではなく、各文書中の
文である。システムはインデックスする文書を個々の文に分解し、
これらの文から bi-gram を取得してインデックスする。
したがって、実際のインデックスには、
「<code>あさ</code>」が含まれるすべての文、
「<code>さっ</code>」が含まれるすべての文、
そして「<code>って</code>」が含まれるすべての文が、
それぞれの bi-gram に対応づけられて格納されている
(注: 実際に格納されているのは文の文字列そのものではなく、
各文への参照IDである)。


<a name="overall"><h4>2.1. 検索の流れ</h4></a>
<p>
例として、「あさっては」という読みが含まれる文:
<center>
「<code>明後日は試合だ。</code>」
</center>
<p>
を検索することを考えてみよう。
システムはまずこの文を読みがなの bi-gram 列に分解し、
インデックスしておく。とりあえず、システムは
<p>
<center>
「<code>明後日</code>」→「<code>あさって</code>」、
「<code>試合</code>」→「<code>しあい</code>」
</center>
<p>
のような単語と読みの対応関係を記述した辞書をもっていると仮定する。
この場合、システムは次のような bi-gram 列を得ることになる:
<p>
<center>
「<code>あさ</code>」
「<code>さっ</code>」
「<code>って</code>」
「<code>ては</code>」
「<code>はし</code>」
「<code>しあ</code>」
「<code>あい</code>」
「<code>いだ</code>」
</center>
<p>
システムはこれらの bi-gram からこの文への写像をインデックスに格納する。
このあとユーザが「<code>あさっては</code>」という検索要求 (読みがな) により
検索をおこなったとすると、システムはまずこの検索語を 3つの bi-gram 
「<code>あさ</code>」「<code>さっ</code>」「<code>って</code>」「<code>ては</code>」に
変換する。つぎにインデックスを参照し、
これら 3つの bi-gram すべてが含まれているような文をとりだす。
この操作を「絞りこみ (narrowing)」と呼ぶことにしよう。
各 bi-gram <em>x</em> に対応する文の集合を S(<em>x</em>) とすると、
これは以下のような集合演算によって計算できる:
<P>
<center>
S("<code>あさ</code>") ∩ S("<code>さっ</code>") ∩ S("<code>って</code>") ∩ S("<code>ては</code>")
</center>
<p>
しかし、これだけでは検索結果としては十分ではない。
もしインデックスに以下のような文章も登録されていたとすると:
<P>
<center>
「<code>浅間山がさっき煙を吹いたってハロルド君が言ってたよ。</code>」
</center>
<p>
ここにも
「<code>あさ</code>」「<code>さっ</code>」「<code>って</code>」「<code>ては</code>」
という bi-gram が現れるからである。
そのためインデックスから返された文集合に対して、
さらなる検査をおこなう必要がある。
しかし重要なことは、このような文はそれほど多くないということである。
したがってインデックスから得られた文に逐次検索をおこなっても、
それらの中には検索要求に実際に一致する可能性が高いものが多く含まれている。
このように、インデックスを使うことにより、検索対象となる文を
絞りこむことができる。
<p>
(注: ここで読者は、連続した箇所に表われる bi-gram のみを
対象にすればさらに対象となる文が絞りこめると考えるかもしれない。
しかし、実際には「<code>明後日</code>」という単語は
「みょうごにち」とも読めるため、実は
本システムではこれらの読みがなに対応する bi-gram 列
「みょ」「ょう」「うご」「ごに」「にち」もインデックスされている。
これら 2つの読み (「あさって」「みょうごにち」) には特に
優先順位がないので、これらの bi-gram が実際にどのような順序で
現れるべきかをシステムが決定することはむずかしい。
「あさ」「さっ」「って」の部分はたしかに連続した bi-gram 列として
インデックス中に現れるかもしれないが、「ては」の部分は
すこし離れた位置に格納されているかもしれない。
そもそも「<code>明後日は</code>」という文字列のどこに
「あさ」「さっ」「って」という bi-gram が現れるのかを
厳密にいうのはむずかしい。なぜなら「明後日」という熟語は
それ全体をさして「あさって」と読むのであって、「明」「後」「日」の
各読みがぞれぞれ「あさ」「さっ」「って」に分解できる、
というものではないからである。このような理由により、
本システムではインデックスを使った絞りこみの時には bi-gram の
順序を考慮せず、たんなる文字列の集合として扱う。これ以降の
厳密な絞りこみは次で述べる逐次検索によっておこなうものとする。)
<p>
さて、対象となる文を絞りこんだあとは、それらすべてに対して、
本当に検索要求にマッチする文を逐次検索で調べる必要がある。
この操作を「マッチング (matching)」と呼ぶ。
本システムでは可能なすべての読み方での検索を許しているので、
「<code>明後日</code>」という文字列は「あさって」でも「みょうごにち」でも
検索できるようにしたい。このように複数存在する漢字の読みを、
単一の読みがなに効率よくマッチさせる作業は、本質的には構文解析における
chart parsing と同じである。すなわち、ユーザが入力した読みがなを
終端記号列として、
<ul>
<li> <code>明</code> → みょう
<li> <code>後</code> → ご
<li> <code>日</code> → にち、じつ
<li> <code>明後日</code> → あさって
</ul>
という導出規則から、「<code>明後日</code>」が
「あさって」あるいは「みょうごにち」を
導出できるかどうかを調べればよい。この操作をインデックスによって
絞りこまれた各文に対しておこなえば、検索は完了する。
<p>
以下の節では読みがな検索のためのインデクシングとマッチングを
それぞれ現実的な時間および記憶容量でおこなうためのアルゴリズムを示す。

<hr noshade>
<a name="indexing"><h3>3. インデクシングのアルゴリズム</h3></A>
<p>
インデクシングとは、インデックスを作成する作業のことである。
本システムでは、ある漢字列があらかじめ用意された
なんらかの辞書によって、可能なすべての読み (ひらがな) に展開できるという
仮定にもとづいている。辞書ファイルにない読みでは検索することができない。
ただし、実際にはすべての読みがなを辞書に入れておく必要はない。
上の例であげた「明」「後」「日」のそれぞれの読み方:
<ul>
<li> <code>明</code> → みょう
<li> <code>後</code> → ご
<li> <code>日</code> → にち、じつ
</ul>
<p>
があれば、「明後日」→「みょうごにち」という読みを登録しておく必要はない。
なぜならシステムは可能なあらゆる読み方を考慮するため、
上の 3つの読み方を知っていれば「明後日」→「みょうごにち」という読みは、
可能な読みとして導出できるからである。同様に「明日 (みょうにち)」
「後日 (ごじつ)」といった読み方も導出できる。
ただし、上の規則からは「明日」を「あした」、
「明後日」を「あさって」と読むような読み方は
導出できないため、このような特殊な読みに対しては
個別に辞書に登録しておく必要がある。
このように、本システムで使う読み方の辞書データは
一般のかな漢字変換辞書に比べて大幅に縮小することができるが、
この具体的な方法については後述する。
<p>
さて、具体的なインデクシングの手続きの説明に入る。
文字列「<code>明後日は</code>」が入力されたとき、システムは
これを先頭から読んだときに可能な読み方をすべて辞書ファイルから列挙する。
これは「<code>明</code>」「<code>明後</code>」「<code>明後日</code>」
「<code>明後日は</code>」といった文字列をキーとして辞書を探索し、
読み方が見つかる可能な最大列まで 1文字ずつキーを伸ばしながら探索を続ける。
ここでは
<ul>
<li> 「<code>明</code>」→ みょう
<li> 「<code>明後日</code>」→ あさって
</ul>
という 2つの候補が発見されたとしよう。システムはこれらの読み方を
bi-gram に分解して出力する。つまり「みょ」「ょう」「あさ」「さっ」「って」
の 5種類の bi-gram が出力されるわけである。
<p>
(注: なお、本システムで使っている「読みがな」の単位は、
読みがなを仮名で書き表したときの文字列であり、音韻学でいうところの
「モーラ」ではない。「<code>今日</code>」の読み「きょう」は
実際には 2モーラであるが、本システムでは 3文字の読みとして扱われる)
<P>
つぎにシステムは入力文の 2文字目から同じ処理を開始する。
つまり「<code>後日は</code>」という文字列に対して可能な読みを列挙するのである。
ここでは
<ul>
<li> 「<code>後</code>」→ ご
</ul>
という候補が発見されたとしよう。システムはこの読み方を bi-gram として
出力するのであるが、読み方は前の文字から連続しているため、
ここではひとつ前の文字の読みを含んだ「○ご」という bi-gram を出力しなければ
ならない。ここで ○ の部分に入る文字は何だろうか?
この前の段階で、「明 (みょ<u>う</u>)」という読みが判明しているので、
ここではその最後の文字「う」がそれに該当する。つまりシステムは
「<code>うご</code>」という bi-gram を出力しなければならない。
いっぽう、「明後日 (あさって)」のほうは、まだ文字列の終わりに達していないため、
「あ」「さ」「っ」「て」のどの文字も「<code>ご</code>」のひとつ前の
読みとしては適当でない。したがって、「<code>後</code>」の bi-gram を生成する際には
「明後日 (あさって)」という読みは考えなくてよいことになる。
<P>
つぎに「<code>日</code>」である。システムは以下のような
辞書エントリをもっていると仮定する:
<ul>
<li> 「<code>日</code>」→ にち、じつ
</ul>
システムはここから得られる 2つの bi-gram 「にち」と「じつ」を
それぞれ「<code>日</code>」の読みとして出力するが、
ここでもひとつ前の文字からの読みを含んだ「○に」「○じ」という bi-gram を
出力しなければならない。ひとつ前の文字「後」に対応する読みは
「ご」であるので、「<code>ごに</code>」「<code>ごじ</code>」という bi-gram が
出力される。
<P>
さて、最後の文字「<code>は</code>」に対してはどうだろうか?
これは漢字ではないので、可能な読みは「は」 1通りだけである。
(注: しかし後述するように、本システムでは点字規則を考慮して
これを「わ」としても読めるようにインデックスしているが、
説明を単純にするためここでは無視する。)
したがって、システムは「○は」という形の bi-gram を出力しなければならない。
○ の部分に対応する文字は、「日 (に<u>ち</u> / じ<u>つ</u>)」の読みの最後の文字、
つまり「ち」と「つ」である。さて、以前に取得した「明後日 (あさっ<strong>て</strong>)」の
読みもこの時点で終端に達していることに注意してほしい。
したがって、○ の部分に入るのは「ち」「つ」「て」の3文字ということになる。
すなわちシステムは「<code>ちは</code>」「<code>つは</code>」「<code>ては</code>」の
3つの bi-gram を出力する。
<p>
以上の手続きにおいて、各文字列のつながりを図にしたものが以下の図 1. である。
赤線の流れが「可能な読み方」の bi-gram に該当する。
<p>
<img src="indexing.png"><br>
図1. 「<code>明後日は</code>」から得られる読みがなの bi-gram 
<p>

以上の説明からわかるように、与えられた文字列の読みに対応する bi-gram を
すべて生成するような手続きは、各文字 C<sub><i>i</i></sub> を処理する
段階において、
「現在注目している文字 C<sub><i>i</i></sub> の、
ひとつ前の文字に対応する読み方の、最後の文字」
のみを記憶していればよいということがわかる。
この「各文字に対応する読み方の、最後の文字」の集合を格納しておくバッファを
L<sub><i>i</i></sub> としよう。上の図を使って説明すると、
システムが「<code>明</code>」および「<code>明後日</code>」に対応する
読みを取得した段階で、すでに L<sub>0</sub> には「<code>う</code>」が、
L<sub>2</sub> には「<code>て</code>」が格納できる。
このようにして、各段階でシステムは L 内の文字を蓄積していくが、
実際に各文字 C<sub><i>i</i></sub> を処理するときに使うのは
L<sub><i>i-1</i> だけである。システムはここに格納されている
文字の集合を使って、おのおのの文字 (読みがな) と、現在の文字の読みがなの
先頭の 1文字を使った bi-gram を生成すればよい
(例: <code>う+ご</code>、 <code>ご+に</code>、 <code>ご+じ</code>、etc.)。
<p>
重要なことは、上で示した bi-gram を生成する過程において
文字列の可能な読みの組み合わせが起こるのは、
各文字の境目 (「<code>後</code>」と「<code>日</code>」、
「<code>日</code>」と「<code>は</code>」) だけであって、
これらの局所的な組み合わせはそれ以外の文字に伝播しないということである。
また「明後日 (あさって)」などのひと続きの読みから bi-gram を生成する場合は、
組み合わせを考慮しなければならないのは先頭と最後の文字だけである。
(注: この特徴は bi-gram のかわりに tri-gram を使ったさいにはあてはまらない。)
このようにして、指数的な爆発を抑えつつ可能な読みの bi-gram をすべて
列挙することができる。
<p>
この手続きは以下のように帰納的に定義することができ、したがって
各 L<sub><i>i</i></sub> の値をバッファとした動的計画法の問題として
とらえることができる:
<ol>
<li> ある文字 C<sub><i>i</i></sub> を処理するとき、
C<sub><i>0</i>...<i>i-1</i></sub> までの文字列の bi-gram はすべて
列挙されたと仮定する。
<li> C<sub><i>i</i></sub> から始まる文字列の可能な読み
C<sub><i>i</i>...<i>i+n</i></sub> の bi-gram をすべて列挙するためには、
その部分文字列に対応するすべての読みの (独立した) bi-gram と、
C<sub><i>i-1</i></sub> を含む読みの最後の文字 (つまり
L<sub><i>i-1</i></sub>) がわかっていればよい。
</ol>
<P>
擬似言語を用いた本アルゴリズムの手続きを以下に示す:
<blockquote><pre>
# input: <i>N</i>文字の入力文字列
input = [C<sub>0</sub>, .... C<sub><i>N-1</i></sub>]
# バッファL: <i>N</i>個の空集合
L = [ φ, φ, ... φ ]

for i = 0 ... <i>N-1</i> {
  # 読みがなの集合を辞書から取得
  Y = lookup_yomi(C<sub><i>i</i></sub>, C<sub><i>i+1</i></sub>, ..., C<sub><i>N-1</i></sub>)
  for each y ∈ Y {
    # y は読みがな文字列で、長さ <i>n</i> をもつ
    if 0 &lt; i {
      # ひとつ前の文字から続く bi-gram を出力
      for each c in L<sub><i>i-1</i></sub> {
        emit_bigram(c ・ y<sub>0</sub>)
      }
    }
    for j = 0 ... <i>n-2</i> {
      # bi-gram をひとつ出力
      emit_bigram(y<sub><i>j</i></sub> ・ y<sub><i>j+1</i></sub>)
    }
    # 最後の1文字をバッファに記録
    L<sub><i>i+n-1</i></sub> = L<sub><i>i+n-1</i></sub> ∪ y<sub><i>n-1</i></sub>
  }
}
</pre></blockquote>

<hr noshade>
<a name="matching"><h3>4. マッチングのアルゴリズム</h3></a>
<p>
前述したように、本システムにおける検索処理は、
「絞りこみ」および「マッチング」の2つの段階に分けられる。
「絞りこみ」部分は、前述したように、入力された読みがな文字列の
bi-gram すべて含む文をインデックスから取り出す処理である。
その後、得られた各文に対して、それらと検索文字列とのマッチングを
おこない、その文が本当に検索条件に合致するかどうかを判定する。
<p>
マッチング部分のアルゴリズムは、構文解析における chart parsing と
本質的に同じである。Chart parsing では、構文解析は与えられた終端記号列を
まとめるような階層的な「弧 (arc)」を生成していくことによって行われる。
ここで与えられた読みがな文字列は終端記号列であり、
検索対象となる文の各文字は非終端記号に相当する。通常の chart parsing と
異なる点は、解析のための文法規則が 1段階しか (明示的には) 与えられていないことと、
終端記号列 (検索読みがな文字列) が文を部分的にしか
カバーしていないことである。
<p>
「解析のための文法規則が明示的に与えられていない」とは、
どういうことなのか? たとえば以下の辞書:
<ul>
<li> <code>明</code> → みょう、めい
<li> <code>後</code> → ご、うしろ
<li> <code>日</code> → にち、じつ、ひ
</ul>
を「非終端記号 → 終端記号列」という形の文法規則と考えよう。
しかし実際のマッチング時に用いる規則の多くは、ここでは省略されている。
それらは、
<ul>
<li> 明後 → 明 + 後
<li> 明日 → 明 + 日
<li> 後日 → 後 + 日
</ul>
といったものである。マッチングにおける暗黙のルールは、
複数の漢字からなる文字列の読みがなについて、
それら各部分の読みをどのように組み合わせていてもかまわない、
というものである。これは構文解析においては、
いかなる<u>非</u>終端記号の組み合わせも許す、ということを意味する。
したがって、通常の chart parsing においては活性弧を生成する際に
それがどの文法規則に相当するものであるかを記録しておく必要があるが、
マッチング処理ではそのような記録は必要ない。
<p>
インデクシングと同様に、マッチング処理もまた
動的計画法の一種とみなせる。Chart parsing においては、ひとつの弧は
「この部分の解析を、ある文法規則によっておこなった」ことを表しているが、
ここでは文法規則を考慮しないので、マッチングにおける弧は
「この部分までの解析は、(使用した文法規則にかかわらず) すべて完了した」ことを
表せる。したがって、バッファに記憶しておいた弧を繰り返し延長することで、
すべての読み (文法規則) を考慮したマッチングがおこなえる。
<p>
図2は「<code>明後日は</code>」という
文字列を「<code>みょうごにち</code>」という読みがな (検索文字列) に
マッチさせる処理である。まず最初の段階 (i=0) で、
読みの最初の 3文字「みょう」が「明」にマッチすることが判明する。
そこでシステムは長さ 3文字 (読み) の弧を生成し、A<sub>1</sub> に格納する。
(なお、このとき弧の開始位置はつねに検索文字列の先頭なので、
弧の開始位置を記録しておく必要はない。)
次の段階 (i=1) では、システムは A<sub>1</sub> の内容を調べ、
これらの弧のいずれかが延長できるかどうかを判断する。
ここでは「ご」が「後」に相当するので、システムは A<sub>1</sub> の
弧を延長した長さ4の弧を A<sub>2</sub> に格納する。
このとき、A<sub>1</sub> にあった元々の長さ 3 の弧は、これ以外の
「後○」で始まる単語が再利用できるよう、そのまま残しておく。
最後に (i=2)、システムは A<sub>2</sub> の弧を延長し、
「みょうごにち」に相当する弧が完成する。このような弧が
バッファ中にひとつでもあれば、その文字列は読みがなにマッチしたことになる。
<p>
<img src="matching.png"><br>
図2. 「<code>明後日は</code>」という文字列を「<code>みょうごにち</code>」にマッチさせる処理
<p>
以下に実際のマッチング処理のアルゴリズムを示す:
<blockquote><pre>
# input: <i>N</i>文字の入力文字列
input = [C<sub>0</sub>, .... C<sub><i>N-1</i></sub>]
# query: <i>M</i>文字の読みがな文字列
query = [q<sub>0</sub>, .... q<sub><i>M-1</i></sub>]
# A: <i>N</i>個の配列。各要素は (長さ0の)弧 1個だけからなる集合。
A = [ {(0,0)}, {(1,0)}, ... {(<i>N-1</i>,0)} ]  # (start,length)

for i = 0 ... <i>N-1</i> {
  # 読みがなの集合を辞書から取得。
  Y = lookup_yomi(C<sub><i>i</i></sub>, C<sub><i>i+1</i></sub>, ..., C<sub><i>N-1</i></sub>)
  for each y ∈ Y {
    # y は読みがな文字列で、長さ <i>n</i> をもつ。
    for each a ∈ A[<i>i</i>] {
      # a はひとつの弧で、a.start から開始し、a.length 文字ぶんの読みに一致。
      if q<sub><i>j</i></sub>q<sub><i>j+1</i></sub>...q<sub><i>j+n-1</i></sub> == y<sub>0</sub>y<sub>1</sub>...y<sub><i>n-1</i></sub> {
        # 読みがなの文字数分だけ弧を延長し、それを配列に加える。
        k = y.kanji_length
        a' = (a.start, a.length+<i>n</i>)
        A[a.start+k] = A[a.start+k] ∪ a'
      }
    }
  }
}

for i = 0 ... <i>N-1</i> {
  for each a ∈ A[<i>i</i>] {
    # すべての読みを覆うような弧が存在するか?
    if a.length == M {
      start = a.start
      end = i
      # C<sub>start</sub>...C<sub>end</sub> が読みにマッチする
      match_succeed(start, end)
    }
  }
}
</pre></blockquote>

<hr noshade>
<a name="optimization"><h3>5. 辞書ファイルの最適化</h3></a>
<p>
本システムで使用する読みがな辞書は、原理的には、
かな漢字変換用の辞書をそのまま使用できる。しかし本システムでは
各漢字の読みがなの組み合わせをすべて考慮するため、
多くの自明な読みをもつ熟語は辞書に登録しておく必要がない。たとえば、
「明 → みょう」「日 → にち」というエントリが元の辞書に
登録されていれば、「明日 →みょうにち」というエントリは
削除することができる。このようにして、本システムでは使用する辞書の
エントリを大幅に削減できる。辞書エントリの削減は、
辞書サイズの縮小と、探索効率の向上という効果をもたらす。
<p>
本記事では読みがな辞書として、Wnn プロジェクトで作られた
パブリックドメインの辞書 pubdic+ を使用している。この辞書にはもともと
43865エントリ (漢字→読みがなの対) があったが、余分なエントリを削除した結果、
最適化された辞書は元の辞書のほぼ 3分の1 にあたる 16799エントリにまで
縮小した。
<p>
以下はそのためのアルゴリズムである:
<p>
<blockquote><pre>
# 入力: <i>N</i>個のエントリ (読み y と漢字 s) をもつ辞書
D<sub>in</sub> = {(y<sub>0</sub>,s<sub>0</sub>), (y<sub>1</sub>,s<sub>1</sub>), ... , (y<sub><i>N-1</i></sub>,s<sub><i>N-1</i></sub>)}
# 出力する辞書
D<sub>out</sub> = {}

# 各辞書エントリについて
for each (y<sub><i>i</i></sub>,s<sub><i>i</i></sub>) ∈ D {
  # このエントリを除いた一時的な辞書 D<sub>tmp</sub> を作成。
  D<sub>tmp</sub> = D<sub>in</sub> - {(y<sub><i>i</i></sub>,s<sub><i>i</i></sub>)}
  # それを使って、y<sub><i>i</i></sub> が s<sub><i>i</i></sub> にマッチ可能かどうかを判断する。
  if ! match(y<sub><i>i</i></sub>, s<sub><i>i</i></sub>, D<sub>tmp</sub>) {
    # マッチできなければ、このエントリを必要とみなし出力に登録。
    D<sub>out</sub> = D<sub>out</sub> ∪ (y<sub><i>i</i></sub>, s<sub><i>i</i></sub>)
  }
}
</pre></blockquote>
<p>
なお、この操作は 1回おこなうだけで、すべての 
(複数回の推移的な導出を含む) 不要なエントリを削除できる。
たとえば以下のような辞書規則を考えよう:
<ul>
<li> A → x
<li> B → y
<li> C → z
<li> AB → xy, st
<li> BC → yz, pq
<li> ABC → xyz, stz, xpq
</ul>
このとき、<code>AB → xy</code>, <code>BC → yz</code> および 
<code>ABC → xyz, stz, xpq</code> の各規則は 1回の最適化で削除される。
しかし規則 <code>AB → st</code> および <code>BC → pq</code> が
削除されることはない。
<p>
簡単な証明:
かりに規則 AB → st が削除されたと仮定しよう。すると、
このとき AB → st を導出するような、さらに細かい規則 A → s および B → t が
存在しているはずである。すると ABC → stz は規則 AB → st がなくても
導出可能になる。いっぽう、ABC → stz を導出するのに
どうしても規則 AB → st が必要だったと仮定すると、
この st という文字列の組み合わせは他のどの規則によっても導出できないため、
規則 AB → st が削除されることはありえない。

<hr noshade>
<a name="braille"><h3>6. 点字規則の追加</h3></a>
<p>
さて、本システムのユーザが視覚障害者であることを考えると、
ユーザは検索文字列を点字キー入力によって入力することも考えられる。
点字は通常のひらがな表記とほとんど同じだが、やや異なる規則があり、
助詞の「は」を「ワ」、「とうきょう」のような「o + う」を含む音を
「トーキョー」のように書きあらわす
(<a href="http://www.tohoho-web.com/tenji.htm">参考</a>)。
また、晴眼者のユーザであっても「携帯」を「ケータイ」と入力するといった
場面は十分考えられる。そのため、本システムではこれらの揺れのある
日本語表記をある程度考慮できるようにした。
<P>
通常、音声合成処理において助詞の「は」を「わ」と発音するには、
システムはその文字が助詞であることを認識しなければならない。
しかし検索時においては「<code>は</code>」というすべての文字が
「は」と「わ」の 2種類の音の可能性を持っていると考えてもよい。
本手法では、インデクシング時に任意の文字列に複数の読みを割りあてることが可能である。
これは検索対象が漢字か仮名かにかかわらないので、「は」という文字を
2種類の読みをもつ特殊な漢字とみなしてインデクシングすることができる。
<P>
いっぽう「とうきょう」のようなものは、bi-gram ベースの拡張をおこなう。
「東京」などの文字列を「トーキョー」で検索するためには、おもに 2つの方法がある。
ひとつは、辞書エントリを最初からすべて
「東京 → トーキョー」のように正規化した形で登録しておき、
検索対象文のなかの仮名もインデクシング時に正規化する
(これは正規表現による置換で簡単に実現可能である) という方法である。
ところが、これだと「<code>激しくうまい</code>」のような文字列が
「激しくーまい」に正規化されてしまい、文字列「<code>うまい</code>」による
検索はできなくなってしまう。そのため、本システムはもうひとつの手法である、
補助的な bi-gram の付加をおこなった。これは、インデクシング時に
「トウ」「ョウ」のような bi-gram が現れたら、
「トー」「ョー」のような bi-gram も現れたとみなす、という方法である。
これにより「東京」は「トウキョウ」「トーキョウ」「トウキョー」「トーキョー」の
4種類の読み方をもつ文字列としてインデックスされる。

<hr noshade>
<a name="exp"><h3>7. 評価実験</h3></a>
<P>
本手法の有効性を示すために、この手法を既存の検索システム上に実装し、
簡単な性能評価をおこなった。

<a name="#impl"><h4>7.1. 実装</h4></a>
<p>
日本語文字列検索フレームワーク
<a href="http://www.unixuser.org/~euske/python/fooling/index.html">Fooling</a>
は Python で書かれた検索フレームワークである。
これはインデックス構造として (漢字を含んだ) 文字ベースの bi-gram から
文への写像を使用しているが、bi-gram 以外のキーを使うように
拡張することも可能である。読みがな検索は Fooling 上のインデックス部分および
マッチング部分を変更することにより実装した。なお、任意の長さをもつ
漢字列から読みがなを取りだす処理は
<a href="http://www.unixuser.org/~euske/doc/tcdb/index.html">tcdb</a>
を用いて実装した。実験に使ったシステムの
<a href="http://fooling.tabesugi.net:8080/">デモはこちら</a>。

<a name="docset"><h4>7.2. 実験に用いた文書集合</h4></a>
<p>
評価実験には、日本語HTMLファイル (全260文書、計8.6MBytes) を
検索対象の文書集合として用いた。文書中には 146,493個の文が
含まれており、インデクシングの対象となった有効な文字数は 3,659,028文字あった。

<a name="indexingtime"><h4>7.3. インデクシングの効率</h4></a>
<p>
この実験では、既存の bi-gram による手法と本手法との効率差を示すため、
読みがな展開を使わない (Foolingの既存の2.2-グラムのみによる) インデクシングと、
それに加えて読みがなの展開も使ったインデクシングの2種類の条件で
インデクシングにかかった時間を計測した。
(なお、この実験で使ったマシンはひどく遅いため、
実際のインデクシングにかかった時間そのものは、
あまり参考にならない。)
<ol type=a>
<li> 漢字、ひらがな、カタカナ、英単語のみでインデクシング:<br>
時間: 約7分、インデックスファイルサイズ合計: 39MBytes。
<li> a. に加えて読みがなを含めイ<u>ソ</u>デクシング:<br>
時間: 約15分、インデックスファイルサイズ合計: 62MBytes。
</ol>
<P>
この結果をみると、読みがなを使ったインデクシングは読みがなを
使わないインデクシングに比べて約2倍の時間がかかっていることがわかる。
なお、b. のインデクシングには読みがなを使わない (漢字を使った) bi-gram も
含まれている。b. で生成されたインデックス中の bi-gram を調べてみると、
その内訳は以下のようになっていた:
<ul>
<li> 漢字bi-gram: 166,660種類、のべ 3,561,494箇所。(1文あたり約24個の bi-gram)
<li> 読みがなbi-gram: 5,875種類、のべ 6,046,784箇所。(1文あたり約41個の bi-gram)
</ul>
<p>
このことから、読みがなを展開したあとの bi-gram は漢字に比べて
ほぼ個数にして 2倍になっていることがわかる。
<p>
読みがなの bi-gram の種類が少ないことは問題である。
これについては 8.3 節で述べる。

<a name="searchingtime"><h4>7.4. 検索の効率</h4></a>
<p>
インデクシングと同様に、検索についても
いくつかのキーワードに対して漢字のみを用いた場合と、
読みがなを用いた場合の効率を比較した。

<table>
<tr>
<th>検索キーワード</th>
<th>所要時間 (秒)</th>
<th>絞りこみ数</th>
<th>マッチした数</th>
</tr>
<tr><td>a. <code>祐介</code></td>
<td align=right>0.36</td><td align=right>41</td><td align=right>41</td></tr>
<tr><td>a. <code>明後日</code></td>
<td align=right>0.13</td><td align=right>3</td><td align=right>3</td></tr>
<tr><td>a. <code>計算機科学</code></td>
<td align=right>0.42</td><td align=right>46</td><td align=right>46</td></tr>
<tr><td>a. <code>東京とニューヨーク</code></td>
<td align=right>0.15</td><td align=right>2</td><td align=right>2</td></tr>
<tr><td>a. <code>連邦準備銀行</code></td>
<td align=right>0.14</td><td align=right>3</td><td align=right>3</td></tr>
<tr><td>b. <code>ゆうすけ</code></td>
<td align=right>1.69</td><td align=right>73</td><td align=right>67</td></tr>
<tr><td>b. <code>あさって</code></td>
<td align=right>1.18</td><td align=right>37</td><td align=right>48</td></tr>
<tr><td>b. <code>けいさんきかがく</code></td>
<td align=right>1.72</td><td align=right>46</td><td align=right>46</td></tr>
<tr><td>b. <code>とうきょうとにゅーよーく</code></td>
<td align=right>0.50</td><td align=right>2</td><td align=right>3</td></tr>
<tr><td>b. <code>れんぽーじゅんびぎんこー</code></td>
<td align=right>0.38</td><td align=right>3</td><td align=right>3</td></tr>
</table>

<p>
(TODO: 各段階において、より細かいプロファイルをとること)

<hr noshade>
<a name="future"><h3>8. 残された課題</h3></a>

<a name="filtering"><h4>8.1. 検索結果のランキングおよびフィルタリング</h4></a>
<p>
Fooling では、検索結果はつねに日付順に返される。
「あさひ」という読みを検索すると、「朝日」「旭」といった文字列のほかにも、
「氷」を「ひ」と発音することがあるため、
「<strong>朝、氷</strong>が張っていた」
というような文字列までヒットしてしまう。

<a name="varied"><h4>8.2. 読みがな以外による表記のゆれ</h4></a>
<p>
読みがなだけでは十分でない。
たとえば「ネーテブ」→「ネイティブ (ネイチブ)」のような表記のゆれ、
「国連」→「国際連合」のような略称は扱えない。

<a name="efficiency"><h4>8.3. 検索効率の改善</h4></a>
<p>
本手法において、
インデクシングのための bi-gram の分散が少ないことは問題である。
キーワードの種類が少ないと、それだけインデクシングによる
絞りこみの効果が薄くなり、大規模な文書検索におけるスケールメリットが
低下するからである。
<P>
日本語の仮名文字はたかだか 80種類ほどであり
(「ヰ」「ヱ」および長音記号「ー」などを入れても 85種類である)、
これらの組み合わせによる bi-gram は最高でも 85^2 = 7225個である。
もっとも多かった読みがなの bi-gram は「ョー」および「ョウ」であったが、
これらは 38732文に現れていた。つまり検索対象となる文書全体の
1/4 の文にこの bi-gram が含まれていることになる。
<P>
したがって、数十万文書を対象にした現実的な検索システムでは、
入力された bi-gram からなるべく頻度の低いものを優先して
絞りこみに用いるべきである。さいわい、bi-gram の数が
限られていることは、この点のチューニングをやりやすくするという
利点ももつ (たかだか 7000個の頻度情報であれば、メモリ中に
静的な配列として持っていればよいからである)。また本手法では
検索語に使われる読みがなの bi-gram が、漢字を用いた検索語の
場合にくらべて多くなる傾向があるが、すべての bi-gram を
絞りこみに使うと負荷が高くなってしまう。しかし頻度の低い
bi-gram から順に絞りこみに使っていき、候補となる文がある
一定数以下になった時点で絞りこみを中止するなどといった
処置をとれば、比較的効率のよい検索を実現できる可能性がある。


<hr noshade>
<address>Yusuke Shinyama</address>
</body>
